# Base parameters for ExPert datasets merge
## I/O options:
datasheet: 'resources/datasets/meta/filtered_datasets_hg38.csv'
output_dir: 'results'                       # Final pipeline outputs, i.e. merged dataset (h5ad), filtered cell meta, and highly variable gene information will be saved to this directory
cache_dir: 'data_cache'                     # NOTICE: Datasets will be downloaded to this directory and multiple copies of it will be saved here, so make sure there is enough disk space
log_dir: 'logs'
plot_dir: 'plots'                           # Change to '' if you don't want meta overview plots
## Pre-processing:
perturbation_col: 'perturbation'            # Column name to look for in .obs that specifies the perturbation label
ctrl_key: 'control'                         # Column value in .obs[perturbation_col] that represents control cells
use_perturbation_pool: true                # Filter every dataset for biggest common set of perturbations
use_feature_pool: true                      # Filter for shared features across all datasets
qc: true                                    # Do quality control on cells (e.g. check for percentage of mitochondiral counts)
norm: false                                 # Apply sc.pp.normalize_total()
log_norm: false                             # Apply log1p scaling
scale: false                                # Center and scale the data
hvg: true                                   # Calculate highly variable genes
n_hvg: 2000                                 # Number of highly variable genes to filter for (required if raw data is used)
subset_hvg: false                           # Directly subset to hvgs (not recommended)
n_ctrl: 10000                               # Maximum number of control cells, randomly draw n_ctrl if there are more control cells than that, keep all if less
single_perturbations_only: true             # Exclude multiple perturbations from dataset
z_score_filter: false                        # Filter cells based on euclidian distance to mean control expression in dataset
control_neighbor_threshold: 0.0               # Filter cells based on percentage of neighbors that are from the same class, set to 0.0 to ignore
min_cells_per_class: 50                     # Filter perturbations for a minimum number of cells

## Mixscale filter:
mixscale_filter: false                       # Filter cells by knockdown efficiency (based on mixscale: https://www.biorxiv.org/content/10.1101/2024.01.29.576933v2)
min_deg: 5                                  # Minimum amount of differentially expressed genes compared to control cells for each perturbation to pass filtering
ctrl_dev: 0.5                               # Minimum standart deviation of perturbed cells mixscale score to control

## Merge & Harmonize:
zero_padding: false                          # Keep genes that are not overlapping with genes from other experiments (assign 0 to them)
correction_method: 'skip'                   # How to harmonize the data over the different experiments, can be one of: 'scANVI', 'scanorama', 'harmonypy', 'skip'
merge_method: 'dask'                        # Whether to merge on-disk (dask) or in memory (memory)
var_merge: 'union'                   # Choose intersection or union of genes for merged dataset
## Pre-computed embeddings
gene_embedding: ''                          # Set path to gene embedding to add to meta-set, will be used to describe perturbations
context_embedding: ''                       # Set path to context embedding TODO: (scTab will be used by default)
add_emb_for_features: false                 # Whether to add embedding information on a feature level

## Other:
cache: true                                 # Whether to cache existing runs or re-run everything
cores: 20                                   # Maximum number of cores to use for the run
plot: false                                 # Whether to plot the entire merged dataset
do_umap: false
do_tsne: false
seed: 42                                    # Set seed for reproducibility

## Slurm config:
# Set RAM limits for all tasks that require loading datasets into memory
resources:
  time:
    short: '4:00:00'
    medium: '24:00:00'
    long: '48:00:00'
    default: '48:00:00'

  memory:
    min_mem: 20                 # Minimum RAM (GB)
    default: 100
    max_mem: 220                # Maximum RAM (GB) of default partition, tries to switch to high_mem if more is needed

  partitions:                   # Set your slurm partitions here
    default: 'normal'                 
    high_mem: ''
    gpu: ''

  jobs:
    download_dataset:
      time: '48:00:00'
      mem: '20GB'

    meta_info:
      time: '48:00:00'
      mem: '20GB'

    process_dataset:
      time: '48:00:00'

    setup_mixscale:
      time: '4:00:00'
      mem: '20GB'

    filter_cells_by_efficiency:
      time: '48:00:00'

    determine_hvg:
      time: '4:00:00'
      mem: '20GB'

    build_gene_pool:
      time: '4:00:00'
      mem: '20GB'

    prepare_dataset:
      time: '48:00:00'

    merge_datasets:
      time: '48:00:00'
      mem: '200GB'

    harmonize:
      time: '48:00:00'
      mem: '200GB'

    add_gene_embedding:
      time: '48:00:00'
      mem: '200GB'

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbdf436e-6db0-46ad-9e45-88ccfa6cdd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable version warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=FutureWarning)\n",
    "# Package setup\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import scanpy as sc\n",
    "\n",
    "# Change to source directory\n",
    "os.chdir('../')\n",
    "# Import expert functions\n",
    "from src.utils.constants import TRAINING_KEYS\n",
    "from src.models._jedvi import JEDVI\n",
    "# Import model run functions\n",
    "import src.utils.io as io\n",
    "from src.tune.run import train, full_run\n",
    "\n",
    "# Setup logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b6dbfe",
   "metadata": {},
   "source": [
    "### I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ee4fe96-34ac-4706-ae35-77cc4fdda10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training and testing data directory\n",
    "work_dir = '/home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069'                 # Replace with path to your data directory\n",
    "train_dir = os.path.join(work_dir, 'train')\n",
    "test_dir = os.path.join(work_dir, 'test')\n",
    "# Set model output directory\n",
    "model_dir = os.path.join(work_dir, 'models', 'small')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "# Set training path\n",
    "train_p = os.path.join(train_dir, 'shared_model_set_100.h5ad')                      # Select a training adata\n",
    "# Set testing path\n",
    "test_p = os.path.join(test_dir, 'jurkat.h5ad')                                      # Select a testing adata\n",
    "# Set path to config\n",
    "config_p = '../resources/params/runs/test/test.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c9d6d2-68c4-4ff8-9e7c-80342d1a8a57",
   "metadata": {},
   "source": [
    "### Two stage training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb461bf-a595-485e-9605-bd8c116d7e6c",
   "metadata": {},
   "source": [
    "#### Stage 1: Pre-train on ELBO + supervised contrastive loss only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48a1569f-0a74-45a3-998f-1487d1d8720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ec1f01c-c694-4c82-b52f-0006acb0b011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 12:10:16,745 - INFO - Loading config file: ../resources/params/runs/two-stage/pre-train.yaml\n",
      "2025-10-28 12:10:16,768 - INFO - Run output directory: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/models/small/pre-train\n",
      "2025-10-28 12:10:16,774 - INFO - Reading training data from: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/train/shared_model_set_100.h5ad\n",
      "2025-10-28 12:10:20,949 - INFO - Setting up model.\n",
      "2025-10-28 12:10:20,950 - INFO - Registered class embedding from adata.uns[cls_embedding].\n",
      "2025-10-28 12:10:20,957 - WARNING - Specified control label neg;control is not in adata class labels, ignoring parameter.\n",
      "2025-10-28 12:10:20,963 - INFO - Calculating external class similarities\n",
      "2025-10-28 12:10:21,386 - INFO - Registered class embedding with shape: (100, 3072), using control: False\n",
      "2025-10-28 12:10:21,693 - INFO - Registered classifier.\n",
      "2025-10-28 12:10:21,698 - INFO - Running at: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/models/small/pre-train\n",
      "2025-10-28 12:10:21,699 - INFO - Epochs suggested: 117, training for 800 epochs.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'src.models._jedvi.JEDVI'</span><span style=\"font-weight: bold\">&gt;</span> Model with the following params: \n",
       "n_classes: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>, n_unseen_classes: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">302</span>, use_gene_emb: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, use_full_cls_emb: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "Training status: Not Trained\n",
       "Model's adata is minified?: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'src.models._jedvi.JEDVI'\u001b[0m\u001b[1m>\u001b[0m Model with the following params: \n",
       "n_classes: \u001b[1;36m100\u001b[0m, n_unseen_classes: \u001b[1;36m302\u001b[0m, use_gene_emb: \u001b[3;91mFalse\u001b[0m, use_full_cls_emb: \u001b[3;91mFalse\u001b[0m\n",
       "Training status: Not Trained\n",
       "Model's adata is minified?: \u001b[3;91mFalse\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:363: Skipping 'train_params' parameter because it is not possible to safely dump to YAML.\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c49e9cd25334358ab467705b0ab4b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1026\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:320\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:192\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:270\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:171\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 171\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/core/module.py:1302\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03mthe optimizer.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1302\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py:154\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py:123\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/torch/optim/adam.py:202\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 202\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py:109\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03mhook is called.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:146\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:131\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 131\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:319\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m--> 319\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:323\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 323\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:391\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/p32655/lschwartz/projects/perturbation_prediction/ExPert/vae/src/_train/plan.py:697\u001b[0m, in \u001b[0;36mContrastiveSupervisedTrainingPlan.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_kl_temperature\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_kl_temperature\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    695\u001b[0m     prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    696\u001b[0m )\n\u001b[0;32m--> 697\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconstrastive_loss_weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontrastive_loss_weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_obs_minibatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprog_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malignment_loss_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malignment_loss_weight\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    709\u001b[0m     prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    710\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/core/module.py:504\u001b[0m, in \u001b[0;36mLightningModule.log\u001b[0;34m(self, name, value, prog_bar, logger, on_step, on_epoch, reduce_fx, enable_graph, sync_dist, sync_dist_group, add_dataloader_idx, batch_size, metric_attribute, rank_zero_only)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    497\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find the `LightningModule` attribute for the `torchmetrics.Metric` logged.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m You can fix this by calling `self.log(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, ..., metric_attribute=name)` where `name` is one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_attributes\u001b[38;5;241m.\u001b[39mvalues())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    503\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtraining\n\u001b[0;32m--> 504\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mis_param_in_hook_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataloader_iter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplicit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    506\u001b[0m ):\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    508\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith `def training_step(self, dataloader_iter)`, `self.log(..., batch_size=...)` should be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    509\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/utilities/signature_utils.py:31\u001b[0m, in \u001b[0;36mis_param_in_hook_signature\u001b[0;34m(hook_fx, param, explicit, min_args)\u001b[0m\n\u001b[1;32m     30\u001b[0m     hook_fx \u001b[38;5;241m=\u001b[39m hook_fx\u001b[38;5;241m.\u001b[39m__wrapped__\n\u001b[0;32m---> 31\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetfullargspec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_fx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m args \u001b[38;5;241m=\u001b[39m parameters\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# ignore `self`\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/inspect.py:1365\u001b[0m, in \u001b[0;36mgetfullargspec\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;66;03m# Re: `skip_bound_arg=False`\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;66;03m# getfullargspec() historically ignored __wrapped__ attributes,\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m     \u001b[38;5;66;03m# so we ensure that remains the case in 3.3+\u001b[39;00m\n\u001b[0;32m-> 1365\u001b[0m     sig \u001b[38;5;241m=\u001b[39m \u001b[43m_signature_from_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mfollow_wrapper_chains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mskip_bound_arg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43msigcls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSignature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43meval_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m   1371\u001b[0m     \u001b[38;5;66;03m# Most of the times 'signature' will raise ValueError.\u001b[39;00m\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;66;03m# But, it can also raise AttributeError, and, maybe something\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;66;03m# else. So to be fully backwards compatible, we catch all\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;66;03m# possible exceptions here, and reraise a TypeError.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/inspect.py:2461\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types\u001b[38;5;241m.\u001b[39mMethodType):\n\u001b[1;32m   2459\u001b[0m     \u001b[38;5;66;03m# In this case we skip the first parameter of the underlying\u001b[39;00m\n\u001b[1;32m   2460\u001b[0m     \u001b[38;5;66;03m# function (usually `self` or `cls`).\u001b[39;00m\n\u001b[0;32m-> 2461\u001b[0m     sig \u001b[38;5;241m=\u001b[39m \u001b[43m_get_signature_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__func__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2463\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_bound_arg:\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/inspect.py:2523\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isfunction(obj) \u001b[38;5;129;01mor\u001b[39;00m _signature_is_functionlike(obj):\n\u001b[1;32m   2521\u001b[0m     \u001b[38;5;66;03m# If it's a pure Python function, or an object that is duck type\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m     \u001b[38;5;66;03m# of a Python function (Cython functions, for instance), then:\u001b[39;00m\n\u001b[0;32m-> 2523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_signature_from_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mskip_bound_arg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_bound_arg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _signature_is_builtin(obj):\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/inspect.py:2358\u001b[0m, in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func, skip_bound_arg, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   2357\u001b[0m keyword_only \u001b[38;5;241m=\u001b[39m arg_names[pos_count:pos_count \u001b[38;5;241m+\u001b[39m keyword_only_count]\n\u001b[0;32m-> 2358\u001b[0m annotations \u001b[38;5;241m=\u001b[39m \u001b[43mget_annotations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2359\u001b[0m defaults \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__defaults__\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/inspect.py:167\u001b[0m, in \u001b[0;36mget_annotations\u001b[0;34m(obj, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m    164\u001b[0m TPFLAGS_IS_ABSTRACT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<<\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m--> 167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_annotations\u001b[39m(obj, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    168\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the annotations dict for an object.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    obj may be a callable, class, or module.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m        functools.update_wrapper()) it is first unwrapped.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m pre_train_config_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../resources/params/runs/two-stage/pre-train.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Train model with loaded config file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43madata_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_train_config_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminify_adata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Evaluate model for all data splits\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m/projects/p32655/lschwartz/projects/perturbation_prediction/ExPert/vae/src/tune/run.py:89\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(adata_p, config_p, out_dir, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m step_model_dir \u001b[38;5;241m=\u001b[39m get_ouptut_dir(config_p, output_base_dir\u001b[38;5;241m=\u001b[39mout_dir)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m model: JEDVI \u001b[38;5;241m=\u001b[39m \u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43madata_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madata_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_model_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_model_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/projects/p32655/lschwartz/projects/perturbation_prediction/ExPert/vae/src/tune/run.py:79\u001b[0m, in \u001b[0;36m_train\u001b[0;34m(adata_p, step_model_dir, config, cls_label, batch_key, verbose, **train_kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     78\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_model_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/projects/p32655/lschwartz/projects/perturbation_prediction/ExPert/vae/src/models/_jedvi.py:831\u001b[0m, in \u001b[0;36mJEDVI.train\u001b[0;34m(self, config, minify_adata)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_runner \u001b[38;5;241m=\u001b[39m runner\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m--> 831\u001b[0m \u001b[43mrunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;66;03m# Add latent representation to model\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_latent_variables()\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/scvi/train/_trainrunner.py:96\u001b[0m, in \u001b[0;36mTrainRunner.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_splitter, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_val\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_plan\u001b[38;5;241m.\u001b[39mn_obs_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_splitter\u001b[38;5;241m.\u001b[39mn_val\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_plan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_splitter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_history()\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# data splitter only gets these attrs after fit\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/scvi/train/_trainer.py:201\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], PyroTrainingPlan):\n\u001b[1;32m    196\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\n\u001b[1;32m    197\u001b[0m         action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    198\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    199\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`LightningModule.configure_optimizers` returned `None`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    200\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "# Pre-train config path\n",
    "pre_train_config_p = '../resources/params/runs/two-stage/pre-train.yaml'\n",
    "# Train model with loaded config file\n",
    "model = train(adata_p=train_p, config_p=pre_train_config_p, out_dir=model_dir, verbose=False, minify_adata=False)\n",
    "# Evaluate model for all data splits\n",
    "model.evaluate()\n",
    "# Test model with a unseen dataset\n",
    "model.test(test_adata_p=test_p, cls_label='cls_label', incl_unseen=False, results_mode='save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da67f001-a382-45cb-bd8d-e47bc748780e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m File                                                                                                      \n",
      "         \u001b[35m/gpfs/projects/b1042/GoyalLab/lschwartz/dl/data/merge/8a2a33dfa4d9a069/models/small/sweep/run_71/config/li\u001b[0m\n",
      "         \u001b[35mghtning_logs/version_0/model/\u001b[0m\u001b[95mmodel.pt\u001b[0m already downloaded                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 18:53:30,823 - INFO - Registered class embedding from adata.uns[cls_embedding].\n",
      "2025-10-28 18:53:30,892 - WARNING - Specified control label neg;control is not in adata class labels, ignoring parameter.\n",
      "2025-10-28 18:53:30,899 - INFO - Calculating external class similarities\n",
      "2025-10-28 18:53:31,163 - INFO - Registered class embedding with shape: (100, 3072), using control: False\n",
      "2025-10-28 18:53:31,470 - INFO - Registered classifier.\n"
     ]
    }
   ],
   "source": [
    "# Load a model\n",
    "model_dir = '/gpfs/projects/b1042/GoyalLab/lschwartz/dl/data/merge/8a2a33dfa4d9a069/models/small/sweep/run_71/config/lightning_logs/version_0/model'\n",
    "version_dir = os.path.dirname(model_dir)\n",
    "output_dir = os.path.join(version_dir, 'test')\n",
    "model = JEDVI.load(model_dir, adata=sc.read(train_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09418d70-fd49-4752-ad8c-31851048573a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 16:21:21,594 - INFO - Running model predictions on all data splits.\n",
      "2025-10-28 16:21:21,607 - INFO - Using class embedding: torch.Size([100, 3072])\n",
      "2025-10-28 16:21:38,310 - INFO - Generating reports.\n",
      "2025-10-28 16:21:39,944 - INFO - Plotting evaluation results.\n",
      "/projects/p32655/lschwartz/projects/perturbation_prediction/ExPert/vae/src/utils/plotting.py:212: UserWarning: KDE cannot be estimated (0 variance or perfect covariance). Pass `warn_singular=False` to disable this warning.\n",
      "  sns.kdeplot(\n",
      "2025-10-28 16:22:12,441 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-28 16:22:12,463 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-28 16:22:22,797 - INFO - Saving model without adata to: /gpfs/projects/b1042/GoyalLab/lschwartz/dl/data/merge/8a2a33dfa4d9a069/models/small/sweep/run_71/config/lightning_logs/version_0/model\n",
      "2025-10-28 16:22:23,051 - INFO - Evaluation done.\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(output_dir=version_dir, results_mode=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63baf830-821f-4fdf-b7d3-c1f50d8659ef",
   "metadata": {},
   "source": [
    "#### Stage 2: Fine-tune pre-trained model with CLIP loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170000d5-7dc9-4f64-91c4-c0ddf28c4303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 18:53:31,610 - INFO - Loading config file: ../resources/params/runs/two-stage/fine-tune.yaml\n",
      "2025-10-28 18:53:31,636 - INFO - Registered class embedding from adata.uns[cls_embedding].\n",
      "2025-10-28 18:53:31,642 - INFO - Adata has already been initialized with <class 'src.models._jedvi.JEDVI'>, loading model settings from adata.\n",
      "2025-10-28 18:53:31,810 - INFO - Registered class embedding with shape: (100, 3072), using control: False\n",
      "2025-10-28 18:53:32,116 - INFO - Registered classifier.\n",
      "2025-10-28 18:53:32,118 - INFO - Registered external embedding aligner.\n",
      "2025-10-28 18:53:32,177 - INFO - Successfully froze decoder parameters.\n",
      "2025-10-28 18:53:32,178 - INFO - Epochs suggested: 117, training for 100 epochs.\n",
      "2025-10-28 18:53:32,179 - INFO - Re-using pre-training data split.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'src.models._jedvi.JEDVI'</span><span style=\"font-weight: bold\">&gt;</span> Model with the following params: \n",
       "n_classes: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>, n_unseen_classes: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">302</span>, use_gene_emb: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "Training status: Not Trained\n",
       "Model's adata is minified?: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'src.models._jedvi.JEDVI'\u001b[0m\u001b[1m>\u001b[0m Model with the following params: \n",
       "n_classes: \u001b[1;36m100\u001b[0m, n_unseen_classes: \u001b[1;36m302\u001b[0m, use_gene_emb: \u001b[3;91mFalse\u001b[0m\n",
       "Training status: Not Trained\n",
       "Model's adata is minified?: \u001b[3;91mFalse\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:363: Skipping 'train_params' parameter because it is not possible to safely dump to YAML.\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1524af12c6a748a6afd854e6c674aec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from src.tune._statics import CONF_KEYS\n",
    "# Specify fine-tuning config path\n",
    "fine_tune_config_p = '../resources/params/runs/two-stage/fine-tune.yaml'\n",
    "# Load fine-tune config params\n",
    "fine_tune_config = io.read_config(fine_tune_config_p)\n",
    "# Add logger to config\n",
    "fine_tune_output_dir = os.path.join(version_dir, 'fine-tune')\n",
    "fine_tune_config[CONF_KEYS.TRAIN]['logger'] = pl.loggers.TensorBoardLogger(fine_tune_output_dir)\n",
    "# Get model params\n",
    "model_kwargs = fine_tune_config[CONF_KEYS.MODEL]\n",
    "# Create new model from pre-trained with frozen base encoder and decoder\n",
    "fine_tune_model = JEDVI.from_base_model(model, freeze_modules=['decoder'], check_model_kwargs=False, **model_kwargs)\n",
    "# Train fine-tune model\n",
    "fine_tune_model.train(fine_tune_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f80b4a-779a-43f2-98fb-a9fc08f3ffcb",
   "metadata": {},
   "source": [
    "### Full run for a single config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eb16340",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 20:25:31,816 - INFO - Loading config file: ../resources/params/runs/test/test.yaml\n",
      "2025-10-22 20:25:31,837 - INFO - Run output directory: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/models/medium/test/test\n",
      "2025-10-22 20:25:31,838 - INFO - Reading training data from: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/train/shared_model_set_w_ctrl.h5ad\n",
      "2025-10-22 20:25:42,376 - INFO - Using perturbation direction to classify\n",
      "2025-10-22 20:25:42,376 - INFO - Setting up model.\n",
      "2025-10-22 20:25:42,376 - INFO - Adding class embedding from adata.uns[cls_embedding] to model\n",
      "2025-10-22 20:25:42,388 - INFO - Adding empty control class embedding, will be learned by model.\n",
      "2025-10-22 20:25:42,792 - INFO - Calculating class similarities\n",
      "2025-10-22 20:25:44,969 - INFO - Registered class embedding with shape: (402, 3072), using control: False\n",
      "2025-10-22 20:25:45,208 - INFO - Running at: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/models/medium/test/test\n",
      "2025-10-22 20:25:45,208 - INFO - Epochs suggested: 34, training for 300 epochs.\n",
      "2025-10-22 20:25:45,217 - INFO - Saving model checkpoints to: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/models/medium/test/test/lightning_logs/version_30/checkpoints\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'src.models._jedvi.JEDVI'</span><span style=\"font-weight: bold\">&gt;</span> Model with the following params: \n",
       "n_classes: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">403</span>, n_unseen_classes: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18209</span>, use_gene_emb: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "ctrl_class: neg;control\n",
       "use_learnable_control_emb: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, use_full_cls_emb: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "Training status: Not Trained\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'src.models._jedvi.JEDVI'\u001b[0m\u001b[1m>\u001b[0m Model with the following params: \n",
       "n_classes: \u001b[1;36m403\u001b[0m, n_unseen_classes: \u001b[1;36m18209\u001b[0m, use_gene_emb: \u001b[3;91mFalse\u001b[0m\n",
       "ctrl_class: neg;control\n",
       "use_learnable_control_emb: \u001b[3;91mFalse\u001b[0m, use_full_cls_emb: \u001b[3;91mFalse\u001b[0m\n",
       "Training status: Not Trained\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:363: Skipping 'train_params' parameter because it is not possible to safely dump to YAML.\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b43067585d44b0819b62313895a4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n",
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n",
      "2025-10-22 21:10:32,182 - INFO - Processing train set\n",
      "2025-10-22 21:10:37,082 - INFO - Using class embedding: torch.Size([402, 3072])\n",
      "2025-10-22 21:10:51,071 - INFO - Using class embedding: torch.Size([402, 3072])\n",
      "2025-10-22 21:10:57,451 - INFO - Calculating latent neighbors\n",
      "2025-10-22 21:11:16,454 - INFO - Calculating latent umap\n",
      "2025-10-22 21:13:50,484 - INFO - Plotting train for label: perturbation_direction\n",
      "... storing 'cls_prediction' as categorical\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/sklearn/metrics/_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "2025-10-22 21:13:55,348 - INFO - Plotting train for label: perturbation\n",
      "2025-10-22 21:14:06,726 - INFO - Plotting train for label: dataset\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret value `dataset` for `hue`. An entry with this name does not appear in `data`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mfull_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Path to the .yaml training config\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# Path to the .h5ad training adata\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# Path to the .h5ad testing adata\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Model output directory\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_unseen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# Whether to test zero-shot test classification on perturbations\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/projects/p32655/lschwartz/projects/perturbation_prediction/ExPert/vae/src/tune/run.py:699\u001b[0m, in \u001b[0;36mfull_run\u001b[0;34m(config_p, train_p, model_dir, test_p, test_unseen, load_best, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m def full_run(\n\u001b[1;32m    689\u001b[0m         config_p: str,\n\u001b[1;32m    690\u001b[0m         train_p: str,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    696\u001b[0m     ) -> dict:\n\u001b[1;32m    697\u001b[0m     # Get config name\n\u001b[1;32m    698\u001b[0m     config_name = os.path.basename(config_p).replace('.yaml', '')\n\u001b[0;32m--> 699\u001b[0m     # Set default model output to config directory\n\u001b[1;32m    700\u001b[0m     model_dir = model_dir if model_dir is not None else os.path.dirname(config_p)\n\u001b[1;32m    701\u001b[0m     # Train model with loaded config file\n",
      "File \u001b[0;32m/projects/p32655/lschwartz/projects/perturbation_prediction/ExPert/vae/src/tune/run.py:753\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(adata_p, config_p, out_dir, **kwargs)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/projects/p32655/lschwartz/projects/perturbation_prediction/ExPert/vae/src/tune/run.py:208\u001b[0m, in \u001b[0;36m_train\u001b[0;34m(adata_p, step_model_dir, config, cls_label, batch_key, verbose)\u001b[0m\n\u001b[1;32m    201\u001b[0m logging.info(f'Running at: {step_model_dir}')\n\u001b[1;32m    202\u001b[0m model.train(\n\u001b[1;32m    203\u001b[0m     data_params=config[CONF_KEYS.DATA].copy(), \n\u001b[1;32m    204\u001b[0m     model_params=config[CONF_KEYS.MODEL].copy(), \n\u001b[1;32m    205\u001b[0m     train_params=config[CONF_KEYS.TRAIN].copy(), \n\u001b[1;32m    206\u001b[0m     return_runner=False\n\u001b[1;32m    207\u001b[0m )\n\u001b[0;32m--> 208\u001b[0m # Save results to lightning directory\n\u001b[1;32m    209\u001b[0m results, latent = get_model_results(\n\u001b[1;32m    210\u001b[0m     model=model, \n\u001b[1;32m    211\u001b[0m     cls_labels=cls_labels, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     max_classes=100\n\u001b[1;32m    215\u001b[0m )\n\u001b[1;32m    216\u001b[0m return model, results, latent\n",
      "File \u001b[0;32m/projects/p32655/lschwartz/projects/perturbation_prediction/ExPert/vae/src/plotting.py:434\u001b[0m, in \u001b[0;36mget_model_results\u001b[0;34m(model, cls_labels, log_dir, modes, batch_label, test_adata, save, plot, max_classes, save_ad)\u001b[0m\n\u001b[1;32m    432\u001b[0m calc_umap(latent)\n\u001b[1;32m    433\u001b[0m plot_model_results_mode(latent, mode, batch_key, cls_labels, plt_dir)\n\u001b[0;32m--> 434\u001b[0m \u001b[43mplot_performance_support_corr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_report\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplt_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmode\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_support_corr.svg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m soft_predictions_mode \u001b[38;5;241m=\u001b[39m plot_soft_predictions(model, data\u001b[38;5;241m=\u001b[39mmode_data, soft_predictions\u001b[38;5;241m=\u001b[39msoft_pred, mode\u001b[38;5;241m=\u001b[39mmode, plt_dir\u001b[38;5;241m=\u001b[39mplt_dir)\n\u001b[1;32m    436\u001b[0m soft_predictions\u001b[38;5;241m.\u001b[39mappend(soft_predictions_mode)\n",
      "File \u001b[0;32m/projects/p32655/lschwartz/projects/perturbation_prediction/ExPert/vae/src/plotting.py:142\u001b[0m, in \u001b[0;36mplot_performance_support_corr\u001b[0;34m(summary, o, hue)\u001b[0m\n\u001b[1;32m    139\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# KDE plot\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkdeplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlog_count\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf1-score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommon_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# Scatter plot\u001b[39;00m\n\u001b[1;32m    153\u001b[0m sns\u001b[38;5;241m.\u001b[39mscatterplot(\n\u001b[1;32m    154\u001b[0m     data\u001b[38;5;241m=\u001b[39msummary,\n\u001b[1;32m    155\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_count\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     legend\u001b[38;5;241m=\u001b[39m(hue \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# only show legend if hue is used\u001b[39;00m\n\u001b[1;32m    162\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/seaborn/distributions.py:1669\u001b[0m, in \u001b[0;36mkdeplot\u001b[0;34m(data, x, y, hue, weights, palette, hue_order, hue_norm, color, fill, multiple, common_norm, common_grid, cumulative, bw_method, bw_adjust, warn_singular, log_scale, levels, thresh, gridsize, cut, clip, legend, cbar, cbar_ax, cbar_kws, ax, **kwargs)\u001b[0m\n\u001b[1;32m   1665\u001b[0m levels \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_levels\u001b[39m\u001b[38;5;124m\"\u001b[39m, levels)\n\u001b[1;32m   1667\u001b[0m \u001b[38;5;66;03m# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #\u001b[39;00m\n\u001b[0;32m-> 1669\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43m_DistributionPlotter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1674\u001b[0m p\u001b[38;5;241m.\u001b[39mmap_hue(palette\u001b[38;5;241m=\u001b[39mpalette, order\u001b[38;5;241m=\u001b[39mhue_order, norm\u001b[38;5;241m=\u001b[39mhue_norm)\n\u001b[1;32m   1676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/seaborn/distributions.py:110\u001b[0m, in \u001b[0;36m_DistributionPlotter.__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    106\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    107\u001b[0m     variables\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    108\u001b[0m ):\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/seaborn/_base.py:634\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_ordered \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# TODO Lots of tests assume that these are called to initialize the\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# mappings to default values on class initialization. I'd prefer to\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# move away from that and only have a mapping when explicitly called.\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/seaborn/_base.py:679\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;66;03m# When dealing with long-form input, use the newer PlotData\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;66;03m# object (internal but introduced for the objects interface)\u001b[39;00m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;66;03m# to centralize / standardize data consumption logic.\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 679\u001b[0m     plot_data \u001b[38;5;241m=\u001b[39m \u001b[43mPlotData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m     frame \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mframe\n\u001b[1;32m    681\u001b[0m     names \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mnames\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/seaborn/_core/data.py:58\u001b[0m, in \u001b[0;36mPlotData.__init__\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     53\u001b[0m     data: DataSource,\n\u001b[1;32m     54\u001b[0m     variables: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, VariableSpec],\n\u001b[1;32m     55\u001b[0m ):\n\u001b[1;32m     57\u001b[0m     data \u001b[38;5;241m=\u001b[39m handle_data_source(data)\n\u001b[0;32m---> 58\u001b[0m     frame, names, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe \u001b[38;5;241m=\u001b[39m frame\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m names\n",
      "File \u001b[0;32m~/.conda/envs/pydl/lib/python3.11/site-packages/seaborn/_core/data.py:232\u001b[0m, in \u001b[0;36mPlotData._assign_variables\u001b[0;34m(self, data, variables)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m         err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn entry with this name does not appear in `data`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m \n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# Otherwise, assume the value somehow represents data\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# Ignore empty data structures\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Could not interpret value `dataset` for `hue`. An entry with this name does not appear in `data`."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model\n",
    "results = full_run(\n",
    "    config_p=config_p,              # Path to the .yaml training config\n",
    "    train_p=train_p,                # Path to the .h5ad training adata\n",
    "    test_p=test_p,                  # Path to the .h5ad testing adata\n",
    "    model_dir=model_dir,            # Model output directory\n",
    "    test_unseen=False               # Whether to test zero-shot test classification on perturbations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4b88a6",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c4ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get config name\n",
    "config_name = os.path.basename(config_p).replace('.yaml', '')\n",
    "# Train model with loaded config file\n",
    "train_output = train(adata_p=train_p, config_p=config_p, out_dir=model_dir, verbose=True)\n",
    "# Ouput is a dictionary with these keys\n",
    "TRAINING_KEYS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e39f21-5b4a-4e80-b280-88ac42ee3847",
   "metadata": {},
   "source": [
    "### Train multiple configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94f9124d-bd7a-4b23-8c21-56e9e962f962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../resources/params/runs/grid/cls_ratio.yaml',\n",
       " '../resources/params/runs/grid/shared_dim.yaml',\n",
       " '../resources/params/runs/grid/rl_weight.yaml',\n",
       " '../resources/params/runs/grid/n_negatives.yaml']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run configs\n",
    "model_dir = os.path.join(work_dir, 'models', 'medium')\n",
    "config_dir = '../resources/params/runs/grid'\n",
    "config_ps = glob.glob(f'{config_dir}/**/*.yaml', recursive=True)\n",
    "config_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26b5ca-1976-44f9-9934-d1bc875c70e4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 23:11:39,875 - INFO - Loading config file: ../resources/params/runs/grid/cls_ratio.yaml\n",
      "2025-10-21 23:11:39,894 - INFO - Run output directory: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/models/medium/grid/cls_ratio\n",
      "2025-10-21 23:11:39,897 - INFO - Reading training data from: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/train/full_model_set_w_ctrl.h5ad\n",
      "2025-10-21 23:11:56,596 - INFO - Using perturbation direction to classify\n",
      "2025-10-21 23:11:56,596 - INFO - Setting up model.\n",
      "2025-10-21 23:11:56,596 - INFO - Adding class embedding from adata.uns[cls_embedding] to model\n",
      "2025-10-21 23:11:56,614 - INFO - Adding empty control class embedding, will be learned by model.\n",
      "2025-10-21 23:11:57,039 - INFO - Calculating class similarities\n",
      "2025-10-21 23:11:59,002 - INFO - Registered class embedding with shape: (1347, 3072), using control: False\n",
      "2025-10-21 23:11:59,260 - INFO - Running at: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/models/medium/grid/cls_ratio\n",
      "2025-10-21 23:11:59,261 - INFO - Epochs suggested: 16, training for 300 epochs.\n",
      "2025-10-21 23:11:59,268 - INFO - Saving model checkpoints to: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/models/medium/grid/cls_ratio/lightning_logs/version_0/checkpoints\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'src.models._jedvi.JEDVI'</span><span style=\"font-weight: bold\">&gt;</span> Model with the following params: \n",
       "n_classes: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1348</span>, n_unseen_classes: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17264</span>, use_gene_emb: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "ctrl_class: neg;control\n",
       "use_learnable_control_emb: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, use_full_cls_emb: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "Training status: Not Trained\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'src.models._jedvi.JEDVI'\u001b[0m\u001b[1m>\u001b[0m Model with the following params: \n",
       "n_classes: \u001b[1;36m1348\u001b[0m, n_unseen_classes: \u001b[1;36m17264\u001b[0m, use_gene_emb: \u001b[3;91mFalse\u001b[0m\n",
       "ctrl_class: neg;control\n",
       "use_learnable_control_emb: \u001b[3;91mFalse\u001b[0m, use_full_cls_emb: \u001b[3;91mFalse\u001b[0m\n",
       "Training status: Not Trained\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:363: Skipping 'train_params' parameter because it is not possible to safely dump to YAML.\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f66880fafb445f6986fa286a499d67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:384: `ModelCheckpoint(monitor='f1_score_validation')` could not find the monitored key in the returned metrics: ['train_loss', 'train_loss_step', 'kl_weight', 'kl_weight_step', 'classification_ratio', 'classification_ratio_step', 'class_kl_temperature', 'class_kl_temperature_step', 'constrastive_loss_weight', 'constrastive_loss_weight_step', 'alignment_loss_weight', 'alignment_loss_weight_step', 'T_classifier_logit', 'T_classifier_logit_step', 'T_contrastive_z', 'T_contrastive_z_step', 'T_clip_z', 'T_clip_z_step', 'train_loss_epoch', 'kl_weight_epoch', 'classification_ratio_epoch', 'class_kl_temperature_epoch', 'constrastive_loss_weight_epoch', 'alignment_loss_weight_epoch', 'T_classifier_logit_epoch', 'T_contrastive_z_epoch', 'T_clip_z_epoch', 'elbo_train', 'reconstruction_loss_train', 'kl_local_train', 'kl_global_train', 'contrastive_loss_train', 'clip_z2c_train', 'clip_c2z_train', 'proxy_train', 'energy_train', 'L2_train', 'classification_loss_train', 'accuracy_train', 'f1_score_train', 'epoch', 'step']. HINT: Did you call `log('f1_score_validation', value)` in the `LightningModule`?\n",
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n",
      "2025-10-22 01:01:18,215 - INFO - Processing train set\n",
      "2025-10-22 01:01:30,914 - INFO - Using class embedding: torch.Size([1347, 3072])\n",
      "2025-10-22 01:02:04,649 - INFO - Using class embedding: torch.Size([1347, 3072])\n",
      "2025-10-22 01:02:22,189 - INFO - Calculating latent neighbors\n",
      "2025-10-22 01:03:42,359 - INFO - Calculating latent umap\n",
      "2025-10-22 01:10:47,389 - INFO - Plotting train for label: perturbation_direction\n",
      "... storing 'cls_prediction' as categorical\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/sklearn/metrics/_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "2025-10-22 01:10:58,057 - INFO - Plotting train for label: perturbation\n",
      "2025-10-22 01:12:51,722 - INFO - Plotting train for label: dataset\n",
      "100%|| 10/10 [00:02<00:00,  4.69it/s]\n",
      "2025-10-22 01:14:41,180 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:14:41,190 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:14:41,340 - INFO - Calculating latent neighbors\n",
      "2025-10-22 01:15:45,117 - INFO - Calculating latent umap\n",
      "2025-10-22 01:23:14,120 - INFO - Plotting train for label: perturbation_direction\n",
      "2025-10-22 01:23:15,960 - INFO - Plotting train for label: perturbation\n",
      "2025-10-22 01:23:35,037 - INFO - Plotting train for label: dataset\n",
      "2025-10-22 01:23:36,971 - INFO - Processing val set\n",
      "2025-10-22 01:23:38,309 - INFO - Using class embedding: torch.Size([1347, 3072])\n",
      "2025-10-22 01:23:41,629 - INFO - Using class embedding: torch.Size([1347, 3072])\n",
      "2025-10-22 01:23:43,382 - INFO - Calculating latent neighbors\n",
      "2025-10-22 01:23:48,111 - INFO - Calculating latent umap\n",
      "2025-10-22 01:24:15,383 - INFO - Plotting val for label: perturbation_direction\n",
      "... storing 'cls_prediction' as categorical\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/sklearn/metrics/_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "2025-10-22 01:24:16,602 - INFO - Plotting val for label: perturbation\n",
      "2025-10-22 01:26:01,862 - INFO - Plotting val for label: dataset\n",
      "100%|| 10/10 [00:00<00:00, 34.70it/s]\n",
      "2025-10-22 01:27:31,859 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:27:31,868 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:27:32,003 - INFO - Calculating latent neighbors\n",
      "2025-10-22 01:27:36,939 - INFO - Calculating latent umap\n",
      "2025-10-22 01:28:03,891 - INFO - Plotting val for label: perturbation_direction\n",
      "2025-10-22 01:28:04,282 - INFO - Plotting val for label: perturbation\n",
      "2025-10-22 01:28:21,704 - INFO - Plotting val for label: dataset\n",
      "2025-10-22 01:28:24,966 - INFO - Calculating latent neighbors\n",
      "2025-10-22 01:29:38,121 - INFO - Calculating latent umap\n",
      "2025-10-22 01:37:47,690 - INFO - Plotting class masks for run.\n",
      "... storing 'mode' as categorical\n",
      "2025-10-22 01:41:35,856 - INFO - Saving results to: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/models/medium/grid/cls_ratio/lightning_logs/version_0\n",
      "2025-10-22 01:41:56,322 - INFO - Loading model checkpoint /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/models/medium/grid/cls_ratio/lightning_logs/version_0/checkpoints/epoch=262-step=90239-f1_score_validation=0.0446617566049099/model.pt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m File                                                                                                      \n",
      "         \u001b[35m/home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/models/medium/grid/cls_ratio/lightning_logs/version\u001b[0m\n",
      "         \u001b[35m_0/checkpoints/\u001b[0m\u001b[95mepoch\u001b[0m=\u001b[1;36m262\u001b[0m-\u001b[33mstep\u001b[0m=\u001b[1;36m90239\u001b[0m-\u001b[33mf1_score_validation\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0446617566049099\u001b[0m/model.pt already downloaded    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 01:41:56,736 - INFO - Adding class embedding from adata.uns[cls_embedding] to model\n",
      "2025-10-22 01:41:57,062 - INFO - Adding empty control class embedding, will be learned by model.\n",
      "2025-10-22 01:41:57,480 - INFO - Calculating class similarities\n",
      "2025-10-22 01:41:59,352 - INFO - Registered class embedding with shape: (1347, 3072), using control: False\n",
      "2025-10-22 01:41:59,661 - INFO - Testing model with: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/test/jurkat.h5ad\n",
      "2025-10-22 01:42:04,540 - INFO - Found 1348 trained class embeddings in model.\n",
      "2025-10-22 01:42:04,541 - INFO - Found 18611 total class embeddings in model.\n",
      "2025-10-22 01:42:04,541 - INFO - Found 1139 test perturbation(s).\n",
      "2025-10-22 01:42:04,541 - INFO - Found 1031 shared perturbations between training and testing.\n",
      "2025-10-22 01:42:04,541 - INFO - Found 108 unseen perturbations between training and testing.\n",
      "2025-10-22 01:42:04,541 - INFO - Found 1135 perturbations in model's class embedding.\n",
      "2025-10-22 01:42:06,080 - INFO - Subsetting to training perturbations only, got 1031\n",
      "2025-10-22 01:42:06,333 - INFO - Added jurkat as dataset key.\n",
      "2025-10-22 01:42:06,815 - INFO - Filtered for minimum mixscale score of 4.0, got 41395 cells.\n",
      "2025-10-22 01:42:07,284 - INFO - Found 789 perturbations with at least 10 cells.\n",
      "2025-10-22 01:42:07,285 - INFO - Using model's gene embeddings for inference.\n",
      "2025-10-22 01:42:07,286 - INFO - Adding class embedding from adata.uns[cls_embedding] to model\n",
      "2025-10-22 01:42:08,525 - INFO - Input AnnData not setup with scvi-tools. attempting to transfer AnnData setup\n",
      "2025-10-22 01:42:09,590 - INFO - Using class embedding: torch.Size([1347, 3072])\n",
      "2025-10-22 01:42:11,137 - INFO - Evaluating results for test set. Test output dir: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/models/medium/grid/cls_ratio/lightning_logs/version_0/test/incl_unseen:False_use_fixed_dataset_label:True_min_ms:4.0_min_cpp:10\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "2025-10-22 01:42:12,407 - INFO - Downloading and generating Enrichr library gene sets...\n",
      "100%|| 20/20 [00:17<00:00,  1.13it/s]\n",
      "2025-10-22 01:42:42,259 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "/projects/p32655/lschwartz/projects/perturbation_prediction/ExPert/vae/src/tune/run.py:499: UserWarning: The palette list has more values (2) than needed (1), which may not be intended.\n",
      "  ax = sns.boxenplot(tmp, x='top_n', y='f1-score', hue='is_training_perturbation',\n",
      "2025-10-22 01:42:42,274 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:42:42,518 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "/projects/p32655/lschwartz/projects/perturbation_prediction/ExPert/vae/src/tune/run.py:499: UserWarning: The palette list has more values (2) than needed (1), which may not be intended.\n",
      "  ax = sns.boxenplot(tmp, x='top_n', y='f1-score', hue='is_training_perturbation',\n",
      "2025-10-22 01:42:42,533 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:42:42,842 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:42:42,867 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:42:43,103 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:42:43,111 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:42:43,448 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:42:43,468 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:42:43,843 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:42:43,867 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:42:44,202 - INFO - Testing model with: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/test/jurkat.h5ad\n",
      "2025-10-22 01:42:47,593 - INFO - Found 1348 trained class embeddings in model.\n",
      "2025-10-22 01:42:47,594 - INFO - Found 18611 total class embeddings in model.\n",
      "2025-10-22 01:42:47,594 - INFO - Found 1139 test perturbation(s).\n",
      "2025-10-22 01:42:47,594 - INFO - Found 1031 shared perturbations between training and testing.\n",
      "2025-10-22 01:42:47,594 - INFO - Found 108 unseen perturbations between training and testing.\n",
      "2025-10-22 01:42:47,594 - INFO - Found 1135 perturbations in model's class embedding.\n",
      "2025-10-22 01:42:49,450 - INFO - Added jurkat as dataset key.\n",
      "2025-10-22 01:42:49,968 - INFO - Filtered for minimum mixscale score of 4.0, got 42706 cells.\n",
      "2025-10-22 01:42:50,468 - INFO - Found 831 perturbations with at least 10 cells.\n",
      "2025-10-22 01:42:50,470 - INFO - Using model's gene embeddings for inference.\n",
      "2025-10-22 01:42:50,471 - INFO - Adding class embedding from adata.uns[cls_embedding] to model\n",
      "2025-10-22 01:42:51,829 - INFO - Input AnnData not setup with scvi-tools. attempting to transfer AnnData setup\n",
      "2025-10-22 01:42:52,988 - INFO - Using class embedding: torch.Size([18612, 3072])\n",
      "2025-10-22 01:43:04,857 - INFO - Evaluating results for test set. Test output dir: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/models/medium/grid/cls_ratio/lightning_logs/version_0/test/incl_unseen:True_use_fixed_dataset_label:True_min_ms:4.0_min_cpp:10\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "2025-10-22 01:43:05,862 - INFO - Downloading and generating Enrichr library gene sets...\n",
      "100%|| 20/20 [00:35<00:00,  1.75s/it]\n",
      "/projects/p32655/lschwartz/projects/perturbation_prediction/ExPert/vae/src/plotting.py:42: RuntimeWarning: invalid value encountered in divide\n",
      "  cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
      "2025-10-22 01:44:24,627 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:24,641 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:24,999 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:25,013 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:25,339 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:25,342 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:25,483 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:25,493 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:25,644 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:25,647 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:25,790 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:25,800 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:25,951 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:25,955 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:26,236 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:26,241 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:26,562 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:26,584 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:26,953 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:26,975 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 01:44:27,642 - INFO - Loading config file: ../resources/params/runs/grid/shared_dim.yaml\n",
      "2025-10-22 01:44:27,666 - INFO - Run output directory: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/models/medium/grid/shared_dim\n",
      "2025-10-22 01:44:27,667 - INFO - Reading training data from: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/train/full_model_set_w_ctrl.h5ad\n",
      "2025-10-22 01:44:38,634 - INFO - Using perturbation direction to classify\n",
      "2025-10-22 01:44:38,690 - INFO - Setting up model.\n",
      "2025-10-22 01:44:38,690 - INFO - Adding class embedding from adata.uns[cls_embedding] to model\n",
      "2025-10-22 01:44:38,725 - INFO - Adding empty control class embedding, will be learned by model.\n",
      "2025-10-22 01:44:39,159 - INFO - Calculating class similarities\n",
      "2025-10-22 01:44:41,157 - INFO - Registered class embedding with shape: (1347, 3072), using control: False\n",
      "2025-10-22 01:44:41,415 - INFO - Running at: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/models/medium/grid/shared_dim\n",
      "2025-10-22 01:44:41,415 - INFO - Epochs suggested: 16, training for 300 epochs.\n",
      "2025-10-22 01:44:41,425 - INFO - Saving model checkpoints to: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/models/medium/grid/shared_dim/lightning_logs/version_0/checkpoints\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'src.models._jedvi.JEDVI'</span><span style=\"font-weight: bold\">&gt;</span> Model with the following params: \n",
       "n_classes: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1348</span>, n_unseen_classes: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17264</span>, use_gene_emb: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "ctrl_class: neg;control\n",
       "use_learnable_control_emb: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, use_full_cls_emb: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "Training status: Not Trained\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'src.models._jedvi.JEDVI'\u001b[0m\u001b[1m>\u001b[0m Model with the following params: \n",
       "n_classes: \u001b[1;36m1348\u001b[0m, n_unseen_classes: \u001b[1;36m17264\u001b[0m, use_gene_emb: \u001b[3;91mFalse\u001b[0m\n",
       "ctrl_class: neg;control\n",
       "use_learnable_control_emb: \u001b[3;91mFalse\u001b[0m, use_full_cls_emb: \u001b[3;91mFalse\u001b[0m\n",
       "Training status: Not Trained\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/pytorch_lightning/core/saving.py:363: Skipping 'train_params' parameter because it is not possible to safely dump to YAML.\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9851346177e406a91de828d4b391290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n",
      "2025-10-22 03:36:10,622 - INFO - Processing train set\n",
      "2025-10-22 03:36:24,092 - INFO - Using class embedding: torch.Size([1347, 3072])\n",
      "2025-10-22 03:37:15,027 - INFO - Using class embedding: torch.Size([1347, 3072])\n",
      "2025-10-22 03:37:34,643 - INFO - Calculating latent neighbors\n",
      "2025-10-22 03:38:39,924 - INFO - Calculating latent umap\n",
      "2025-10-22 03:45:52,651 - INFO - Plotting train for label: perturbation_direction\n",
      "... storing 'cls_prediction' as categorical\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/sklearn/metrics/_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "2025-10-22 03:46:03,099 - INFO - Plotting train for label: perturbation\n",
      "2025-10-22 03:47:57,642 - INFO - Plotting train for label: dataset\n",
      "100%|| 10/10 [00:02<00:00,  4.81it/s]\n",
      "2025-10-22 03:49:46,392 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 03:49:46,401 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 03:49:46,558 - INFO - Calculating latent neighbors\n",
      "2025-10-22 03:51:07,552 - INFO - Calculating latent umap\n",
      "2025-10-22 03:58:26,279 - INFO - Plotting train for label: perturbation_direction\n",
      "2025-10-22 03:58:28,135 - INFO - Plotting train for label: perturbation\n",
      "2025-10-22 03:58:47,170 - INFO - Plotting train for label: dataset\n",
      "2025-10-22 03:58:49,076 - INFO - Processing val set\n",
      "2025-10-22 03:58:50,555 - INFO - Using class embedding: torch.Size([1347, 3072])\n",
      "2025-10-22 03:58:54,252 - INFO - Using class embedding: torch.Size([1347, 3072])\n",
      "2025-10-22 03:58:55,950 - INFO - Calculating latent neighbors\n",
      "2025-10-22 03:59:01,860 - INFO - Calculating latent umap\n",
      "2025-10-22 03:59:28,274 - INFO - Plotting val for label: perturbation_direction\n",
      "... storing 'cls_prediction' as categorical\n",
      "/home/xlv0877/.conda/envs/pydl/lib/python3.11/site-packages/sklearn/metrics/_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "2025-10-22 03:59:29,470 - INFO - Plotting val for label: perturbation\n",
      "2025-10-22 04:01:15,799 - INFO - Plotting val for label: dataset\n",
      "100%|| 10/10 [00:00<00:00, 34.92it/s]\n",
      "2025-10-22 04:02:45,632 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 04:02:45,641 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-10-22 04:02:45,769 - INFO - Calculating latent neighbors\n",
      "2025-10-22 04:02:52,447 - INFO - Calculating latent umap\n",
      "2025-10-22 04:03:19,519 - INFO - Plotting val for label: perturbation_direction\n",
      "2025-10-22 04:03:19,900 - INFO - Plotting val for label: perturbation\n",
      "2025-10-22 04:03:37,497 - INFO - Plotting val for label: dataset\n",
      "2025-10-22 04:03:40,784 - INFO - Calculating latent neighbors\n",
      "2025-10-22 04:04:54,159 - INFO - Calculating latent umap\n",
      "2025-10-22 04:13:14,656 - INFO - Plotting class masks for run.\n",
      "... storing 'mode' as categorical\n",
      "2025-10-22 04:17:08,335 - INFO - Saving results to: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/models/medium/grid/shared_dim/lightning_logs/version_0\n",
      "2025-10-22 04:17:29,691 - INFO - Loading model checkpoint /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/models/medium/grid/shared_dim/lightning_logs/version_0/checkpoints/epoch=270-step=92897-f1_score_validation=0.04921649768948555/model.pt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m File                                                                                                      \n",
      "         \u001b[35m/home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/models/medium/grid/shared_dim/lightning_logs/versio\u001b[0m\n",
      "         \u001b[35mn_0/checkpoints/\u001b[0m\u001b[95mepoch\u001b[0m=\u001b[1;36m270\u001b[0m-\u001b[33mstep\u001b[0m=\u001b[1;36m92897\u001b[0m-\u001b[33mf1_score_validation\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.04921649768948555\u001b[0m/model.pt already downloaded  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 04:17:30,209 - INFO - Adding class embedding from adata.uns[cls_embedding] to model\n",
      "2025-10-22 04:17:30,536 - INFO - Adding empty control class embedding, will be learned by model.\n",
      "2025-10-22 04:17:31,048 - INFO - Calculating class similarities\n",
      "2025-10-22 04:17:32,999 - INFO - Registered class embedding with shape: (1347, 3072), using control: False\n",
      "2025-10-22 04:17:33,306 - INFO - Testing model with: /home/xlv0877/proj_home/dl/data/merge/8a2a33dfa4d9a069/test/jurkat.h5ad\n",
      "2025-10-22 04:17:38,234 - INFO - Found 1348 trained class embeddings in model.\n",
      "2025-10-22 04:17:38,234 - INFO - Found 18611 total class embeddings in model.\n",
      "2025-10-22 04:17:38,235 - INFO - Found 1139 test perturbation(s).\n",
      "2025-10-22 04:17:38,235 - INFO - Found 1031 shared perturbations between training and testing.\n",
      "2025-10-22 04:17:38,235 - INFO - Found 108 unseen perturbations between training and testing.\n",
      "2025-10-22 04:17:38,235 - INFO - Found 1135 perturbations in model's class embedding.\n"
     ]
    }
   ],
   "source": [
    "# Train configs\n",
    "for config_p in config_ps:\n",
    "    run_name = os.path.basename(os.path.dirname(config_p))\n",
    "    run_dir = os.path.join(model_dir, run_name)\n",
    "    full_run(config_p=config_p, train_p=train_p, test_p=test_p, model_dir=run_dir, test_unseen=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838d320f-129b-4738-921e-c9e688751f1d",
   "metadata": {},
   "source": [
    "#### Manual model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f304d921-3c23-4d1e-804d-ba194d245132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual model testing\n",
    "\n",
    "# Load best model\n",
    "version_dir = 'model_dir/test/lightning_logs/version_18'\n",
    "model = JEDVI.load_checkpoint(\n",
    "    version_dir,\n",
    "    adata=sc.read(train_p)\n",
    ")\n",
    "# Manually test model with control neighbor filtering\n",
    "results = test(\n",
    "    model, \n",
    "    test_adata_p=test_p, \n",
    "    output_dir=version_dir, \n",
    "    incl_unseen=False, \n",
    "    plot=True, \n",
    "    return_results=False, \n",
    "    min_ms=0.0,\n",
    "    control_neighbor_threshold=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ece0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydl",
   "language": "python",
   "name": "pydl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

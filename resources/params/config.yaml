# Dataloader parameters
data:
  train_size: 0.9                                   # Train split percentage
  batch_size: null                                  # Automatic batch size w. contrastive parameters
  num_workers: 1                                    # Number of data loaders (1 seems to be optimal)
  pin_memory: false                                 # Pin memory
  max_cells_per_batch: 32                           # Number of cells per class in each batch (range: 8 - 64)
  max_classes_per_batch: 16                         # Number of classes in each batch         (range: 8 - 64)
  shuffle_classes: true                             # Shuffle classes while creating batches
  use_contrastive_loader: "train"                   # Set to ["train", "val", "both"]

# Classifier parameters
cls:
  n_hidden: 0                                       # 0 = linear classifier
  n_layers: 1                                       # Number of layers (only used of n_hidden!=0)
  dropout_rate: 0.1                                 # Classifier dropout rate
  use_cosine_similarity: true                       # Use cosine similarity to class embeddings for logits calculation (recommended)
  temperature: 1                                    # Temperature scaling of logits (1)
  skip_projection: false                            # Allows model to skip projection to class embedding and use z directly (if dimensions match)

# Encoder parameters
encoder:
  encoder_type: "transformer"                       # ["transformer", "funnel", "fc"]
  linear_encoder: true                              # Use linear layers while encoding
  n_layers: 0                                       # Number of layers to use in encoder
  # Attention params
  use_attention: true                               # Whether to use attention in encoder
  n_head: 8                                         # Number of attention heads (aim for 16-64 d_h, ideally 32)
  n_attn_layers: 4                                  # Number of attention layers in encoder
  ff_mult: 4                                        # Latent expansion factor (transformer only)
  activation_fn: "nn.LeakyReLU"                     # Activation function
  noise_std: 0.000001                                   # Noisy encoder option

# Decoder parameters
decoder:
  linear_decoder: false                             # Use linear decoder
  n_layers: 1                                       # Number of layers in decoder
  use_attention: false                              # use attention in decoder
  activation_fn: "nn.LeakyReLU"                     # Decoder activation function

# Weight schedules
schedules:
  classification_ratio:                             # Classification loss weight schedule
    schedule: "sigmoid"
    anneal_k: 10
    n_epochs_stall: 0
    min_weight: 1
    max_weight: 1
  contrastive_loss_weight:                          # Contrastive loss weight schedule
    schedule: "sigmoid"
    anneal_k: 10
    n_epochs_stall: 0
    min_weight: 1
    max_weight: 1
  alignment_loss_weight:                            # Latent space alignment loss weight schedule
    schedule: "sigmoid"
    anneal_k: 10
    n_epochs_stall: 0
    min_weight: 1
    max_weight: 1
  class_kl_temperature:                             # KL classification temperature range: [0.01-1]
    schedule: "sigmoid"
    anneal_k: 10
    n_epochs_stall: 0
    min_weight: 0.1
    max_weight: 0.1

# Training plan
plan:
  lr: 0.0005                                        # Learning rate (LR)
  lr_patience: 30                                   # LR patience
  weight_decay: 0.00001                             # Weight decay
  use_full_cls_emb: false                           # Use full class embedding during training or subset to observed classes
  n_epochs_warmup: 300                              # Warmup period for schedules
  min_kl_weight: 0                                  # KL weight schedule
  max_kl_weight: 1
  target_scale: 3.0                                 # Classification KL logit rescaler value , 1==one-hot, >1==class modules
  use_posterior_mean: "val"                         # Use full z or mean for classifier ["train", "val", "both"]
  use_contr_in_val: false                           # Use contrastive loss in validations set
  lr_scheduler_metric: "validation_loss"            # LR metric
  # Tensorboard logging options
  log_class_distribution: false                     # Log class label distribution in batch
  full_val_log_every_n_epoch: 10                    # Log full validation metrics, slows down training a bit if low
  plot_umap: null                                   # Plot umap of latent space ["train", "val", "both", null]

# Train parameters
train:
  max_epochs: 300                                   # Max training epochs
  early_stopping: false                             # Early stopping
  early_stopping_monitor: "validation_loss"         # Early stopping metrics
  early_stopping_patience: 30
  check_val_every_n_epoch: 1

# Model parameters
model:
  n_latent: 128                                     # Latent space dimension (z)
  n_hidden: -1                                      # Start dimension, -1==full gene space, other==1st encoder layer projection
  dropout_rate: 0.3                                 # Overall dropout rate
  dispersion: "gene"
  use_batch_norm: "none"
  use_layer_norm: "both"
  gene_likelihood: "zinb"
  linear_classifier: true                           # Overwrites classifier parameters
  log_variational: true
  l1_lambda: 0.0                                    # Should be 0 when using attention
  l2_lambda: 0.001                                   # Should always be > 0, help prevent overfitting
  l_mask: ["attn", "Attention", "class_projection"] # Use l1 / l2 regularization on these layers
  classification_loss_strategy: "kl"                # Enable KL-based class embedding classification
  contrastive_temperature: 0.1                      # Contrastive logit temperature scaling, lower --> sharper distinction
  reduction: "sum"                                  # ELBO-loss reduction method (i.e. reconstruction & KL)
  non_elbo_reduction: "sum"                         # Non-ELBO-loss reduction method (i.e. classification, contrastive)
  use_feature_mask: false                           # Randomly dropout input features
  drop_prob: 0.001                                  # Feature dropout rate

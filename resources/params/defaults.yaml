# Dataloader parameters
data:
  train_size: 0.9                                   # Train split percentage
  batch_size: null                                  # Automatic batch size w. contrastive parameters
  num_workers: 1                                    # Number of data loaders (1 seems to be optimal)
  pin_memory: false                                 # Pin memory
  max_cells_per_batch: 16                           # Number of cells per class in each batch (range: 8 - 128); lower = global, higher = local pull stronger
  max_classes_per_batch: 4                         # Number of classes in each batch         (range: 8 - 128); lower = local seperation, higher = global
  shuffle_classes: true                             # Shuffle classes while creating batches
  use_contrastive_loader: false                   # Set to ["train", "val", "both"] or null to disable contrastive loading
  ctrl_frac: 0.25                                    # Proportion of control cells in batch (only used if control_key is specified and control data is found in training data)

# Classifier parameters
cls:
  n_hidden: 0                                      # If n_hidden > 0 and n_layers > 0 --> use FCLayers, else linear layer
  n_layers: 1
  dropout_rate: 0.3
  temperature: 1.0                                  # Temperature scaling by which logits are divided
  margin: 0.2                                         # Optional angular margin (only used for ARC-style classifier)

# External class embedding aligner parameters
aligner:
  n_hidden: 0
  n_layers: 1
  dropout_rate: 0.2
  temperature: 0.5
  mode: shared                                  # How to align class and latent space ["shared", "class_to_latent", "latent_to_class"]
  shared_projection_dim: 512                      # Dimensions of shared projection (only used if mode = "shared")
  skip_projection: false                            # Whether to skip a projection if shapes match (only used in "class_to_latent", "latent_to_class")
# Encoder parameters
encoder:
  encoder_type: "transformer"                       # ["transformer", "funnel", "fc"]
  linear_encoder: false                              # Use linear layers while encoding
  n_layers: 1                                       # Number of layers to use in encoder
  # Attention params
  use_attention: false                               # Whether to use attention in encoder
  n_head: 8                                         # Number of attention heads (aim for 16-64 d_h, ideally 32)
  n_attn_layers: 2                                  # Number of attention layers in encoder
  ff_mult: 4                                        # Latent expansion factor (transformer only)
  activation_fn: "nn.LeakyReLU"                     # Activation function
  noise_std: 0.0                                    # Noisy encoder option
  use_film: false                                   # Use FiLM to encode batch embeddings

# Decoder parameters
decoder:
  linear_decoder: false                             # Use linear decoder
  n_layers: 1                                       # Number of layers in decoder
  use_attention: false                              # use attention in decoder
  activation_fn: "nn.LeakyReLU"                     # Decoder activation function

# Weight schedules
schedules:
  rl_weight:                                        # Reconstruction loss weight schedule
    schedule: "sigmoid"
    anneal_k: 10
    n_epochs_stall: 0
    min_weight: 1
    max_weight: 1
  kl_weight:                                        # KL loss weight schedule
    schedule: "sigmoid"
    anneal_k: 10
    n_epochs_stall: 0
    min_weight: 0
    max_weight: 0.25
  classification_ratio:                             # Classification loss weight schedule
    schedule: "sigmoid"
    anneal_k: 10
    n_epochs_stall: 150
    min_weight: 0
    max_weight: 1
  contrastive_loss_weight:                          # Contrastive loss weight schedule
    schedule: "sigmoid"
    anneal_k: 10
    n_epochs_stall: 0
    min_weight: 0
    max_weight: 1
  alignment_loss_weight:                            # Latent space alignment loss weight schedule
    schedule: "sigmoid"
    anneal_k: 10
    n_epochs_stall: 0
    min_weight: 0
    max_weight: 0
  adversarial_context_lambda:                       # Adversial context loss to disentangle context from latent
    schedule: "sigmoid"
    anneal_k: 10
    n_epochs_stall: 0
    min_weight: 0
    max_weight: 0

# Training plan
plan:
  lr: 0.0001                                        # Learning rate (LR)
  lr_patience: 30                                   # LR patience
  weight_decay: 0.00001                             # Weight decay
  n_epochs_warmup: 400                              # Warmup period for schedules (mainly KL)
  multistage_kl_frac: 0.5                           # Fraction of max KL-weight to reset to at an introduction of a new loss
  target_scale: 2.0                                 # Classification KL logit rescaler value , 1==one-hot, >1==class modules
  n_negatives: 128                                  # How many negative classes to randomly sample each epoch
  scale_by_temp: false                                 # Scale KL-classification loss by temperature
  observed_only: false                              # Calculate classification loss on classes in batch only
  use_posterior_mean: "val"                         # Use full z or mean for classifier ["train", "val", "both"]
  use_contr_in_val: false                           # Use contrastive loss in validations set (only compatible if val is also a contrastive dl)
  lr_scheduler_metric: "validation_loss"            # LR metric
  # Tensorboard logging options
  log_class_distribution: false                     # Log class label distribution in batch
  full_val_log_every_n_epoch: 30                     # Log full validation metrics, slows down training a bit if low
  plot_umap: null                                   # Plot umap of latent space ["train", "val", "both", null]
  average: "weighted"                               # How to calculate accuracy and F1 ["weighted", "macro", "micro"]
  top_k: 1                                          # Calculate top k prediction accuracy and F1 for each batch

# Train parameters
train:
  max_epochs: 400                                   # Max training epochs
  early_stopping: true                             # Early stopping
  early_stopping_monitor: "f1_score_validation"     # Early stopping metric
  early_stopping_mode: "max"
  early_stopping_patience: 60
  checkpoint: false                                  # Create model checkpoints
  checkpoint_monitor: "f1_score_validation"
  checkpoint_mode: "max"
  check_val_every_n_epoch: 1

# Model parameters
model:
  n_latent: 64                                     # Latent space dimension (z)
  n_hidden: -1                                      # Start dimension, -1==full gene space, other==1st encoder layer projection
  dropout_rate: 0.4                                 # Overall dropout rate
  dispersion: "gene"
  use_batch_norm: "both"
  use_layer_norm: "none"
  gene_likelihood: "zinb"
  use_full_cls_emb: false                            # Use full class embedding during training or subset to observed classes
  linear_classifier: true                           # Overwrites classifier parameters
  log_variational: true
  l1_lambda: 0.0                                    # Should be 0 when using attention
  l2_lambda: 0.0                                  # Should always be > 0, helps prevent overfitting
  l_mask: ["attn", "projection"]                    # Use l1 / l2 regularization on these layers (word has to be contained in layer name)
  classification_module_type: "standard"                  # Can be either "standard" or "arc"
  classification_loss_strategy: "focal"                   # Can be either "ce" or "focal"
  focal_gamma: 2.0                                          # Only used if strategy = "focal"
  align_ext_emb_loss_strategy: ["clip"]                # Set classification loss strategy: "clip", and/or "kl"
  use_learnable_control_emb: false                  # Whether to use learnable control embedding or use no embedding for control class
  use_learnable_temperature: true                  # Learn own temperature scalings for logit-based losses, useful for large number of classes
  use_adversial_context_cls: false                  # Learn a context classifier and invert its gradient
  # Control cell options
  use_reconstruction_control: false                 # Whether to use control cells for reconstruction loss (not recommended as it focuses too much on reconstructing backgrounds)
  use_kl_control: true                             # Whether to use control cells for KL loss
  use_classification_control: false                  # Whether to use control cells for classification loss (not recommended due to extreme class imbalance)
  use_contrastive_control: true                     # Whether to use control cells for contrastive loss
  ctrl_class: "neg;control"                             # Specify control class, will include a learnable embedding for that class, only specify if control cells are actually in data
  contrastive_temperature: 0.5                      # Contrastive logit temperature scaling, lower --> sharper distinction
  reduction: "mean"                                  # ELBO-loss reduction method (i.e. reconstruction & KL), default is "batchmean"
  non_elbo_reduction: "mean"                         # Non-ELBO-loss reduction method (i.e. classification, contrastive)
  use_feature_mask: false                           # Randomly dropout input features
  drop_prob: 0.001                                  # Feature dropout rate
  batch_representation: "embedding"                   # How to encode batch variables, either "one-hot" or "embedding"
  batch_embedding_kwargs:
    embedding_dim: 2                                  # Number of batch embedding dimensions to learn

# Model setup parameters
model_setup:
  batch_key: "dataset"                              # Context key to normalize for in .obs
  labels_key: "cls_label"                           # Label columns in .obs
  class_emb_uns_key: "cls_embedding"                           # .uns key for class embedding
  gene_emb_varm_key: null                           # .varm key for gene embeddings
